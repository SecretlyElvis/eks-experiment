* Code Repo:
https://github.com/SecretlyElvis/eks-experiment.git

### AWS | resources

AWS Load Balancer Controller Helm Chart:
https://github.com/aws/eks-charts/tree/master/stable/aws-load-balancer-controller
https://artifacthub.io/packages/helm/aws/aws-load-balancer-controller
https://github.com/aws/eks-charts/tree/master/stable/aws-load-balancer-controller

ALB Ingress Controller for EKS:
https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html
https://kubernetes-sigs.github.io/aws-load-balancer-controller/guide/ingress/annotations/

Annotations for ALB Ingress controller:
kubernetes.io/ingress.class: alb
alb.ingress.kubernetes.io/group.name: <my-group> (to add multiple ingress elements to single group)
alb.ingress.kubernetes.io/group.order: <'10'> (to order rules, lower numbers evaluated first)

When defining the load balancer, specify 'Instance' traffic mode (routes to node, then proxies to pods) 
alb.ingress.kubernetes.io/target-type: instance

NOTE: Your Kubernetes service must specify the NodePort type to use this traffic mode.

Ingress Annotation list:
https://kubernetes-sigs.github.io/aws-load-balancer-controller/guide/ingress/annotations/

Ingress IAM policy (downloaded and installed for ALB):
https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2_ga/docs/install/iam_policy.json

Required Tags for Using the ALB Ingress Controller:
https://docs.aws.amazon.com/eks/latest/userguide/network_reqs.html#vpc-subnet-tagging

Test app for ALB Ingress Controller (posted 02-Oct by AWS):
https://aws.amazon.com/premiumsupport/knowledge-center/eks-alb-ingress-controller-setup/

### HELM | ALB Ingress Controller (deprecated as of 13-Nov-2020)

https://github.com/helm/charts/tree/master/incubator

Installation:
* helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator
* helm install incubator/aws-alb-ingress-controller --set clusterName=MyClusterName --set autoDiscoverAwsRegion=true --set autoDiscoverAwsVpcID=true --name my-release --namespace kube-system

- Additional notes for upgrading to v1.0.2:
https://github.com/kubernetes-sigs/aws-load-balancer-controller
https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/master/docs/guide/ingress/annotation.md#wafv2
https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/0338ed144f584c7a7738b4bf1d8ca8c827e7abb0/docs/examples/iam-policy.json#L117-L126

- New IAM permission is required even no wafv2 annotation is used.
- WAFV2 support can be disabled by controller flags --feature-gates=wafv2=false

### HELM | Jenkins

Official Jenkins.io Installing Jenkins on Kubernetes:
https://www.jenkins.io/doc/book/installing/kubernetes/

Jenkins on Kubernetes Engine:
https://cloud.google.com/solutions/jenkins-on-kubernetes-engine

Helm Chart codebase:
https://github.com/jenkinsci/helm-charts

Helm Chart documentation:
https://github.com/jenkinsci/helm-charts/blob/main/charts/jenkins/README.md

https://bitnami.com/stack/jenkins/helm
https://medium.com/faun/deploying-and-scaling-jenkins-on-kubernetes-2cd4164720bd
https://medium.com/appfleet/how-to-set-up-jenkins-on-kubernetes-70f8eac3dc7e
https://hands-on-tech.github.io/2020/03/15/k8s-jenkins-example.html
https://cloud.google.com/solutions/jenkins-on-kubernetes-engine?hl=it
https://www.edureka.co/community/74612/how-to-configure-jenkins-environment-using-helm
https://docs.netapp.com/us-en/project-astra/pdfs/pages/solutions/jenkins-deploy-from-helm-chart.pdf

https://cloudacademy.com/course/introduction-to-helm-1034/helm-charts/

Useful commands:
helm search repo jenkinsci

### HELM | Nexus

https://blog.sonatype.com/how-to-use-nexus-repository-and-helm-for-ci/cd
https://help.sonatype.com/repomanager3/formats/helm-repositories
https://github.com/helm/charts/tree/master/stable/sonatype-nexus (deprecated)
https://stackoverflow.com/questions/52274286/run-nexus-in-kubernetes-cluster-using-helm
https://www.freshbrewed.science/getting-started-with-containerized-nexus/index.html
https://www.sonatype.com/press-release-blog/sonatype-adds-native-helm-support

Possible chart option (Otemo -- GCP-specific):
https://github.com/Oteemo/charts
Docs: https://artifacthub.io/packages/helm/oteemo-charts/sonatype-nexus

helm repo add oteemocharts https://oteemo.github.io/charts
helm install sonatype-nexus -n devops-poc \
  --set ingress.enabled="true" \
  --set ingress.path="/nexus" \
  --set persistence.existingClaim="nexus-prd-pvc" \
  oteemocharts/sonatype-nexus

Kubernetes Nexus:
https://github.com/travelaudience/kubernetes-nexus#pre-requisites

Backing up Nexus Configs and Artifacts:
https://blog.sonatype.com/2010/01/how-to-backup-nexus-configuration-and-repository-artifacts/

Why use Nexus?:
https://blog.sonatype.com/how-to-use-nexus-repository-and-helm-for-ci/cd

Very basic Nexus setup:
https://devopscube.com/setup-nexus-kubernetes/

Official Nexus Repository Docker Image:
https://hub.docker.com/r/sonatype/nexus3

TravelAudience Nexus Repository Docker Image:
https://quay.io/repository/travelaudience/docker-nexus

Setting for 'ingress.path' can be changed in otemo chart (need to find out how to do it as a straight container)

#### RESOURCES ####

-- Pod-to-pod communication in Kubernetes:
https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/

-- Ingress Controllers 101 (HAProxy):
https://thenewstack.io/kubernetes-ingress-for-beginners/

-- Using the HAProxy Ingress controller (good exapmles at the bottom):
https://www.haproxy.com/documentation/hapee/1-5r2/installation/kubernetes-ingress-controller/
https://www.ionos.com/community/markdown-migration/configure-and-manage-a-kubernetes-haproxy-ingress-controller/

Using YAML files: https://www.haproxy.com/blog/dissecting-the-haproxy-kubernetes-ingress-controller/
  (includes complete example of application deploy that tests install)
Using Helm: https://www.haproxy.com/blog/use-helm-to-install-the-haproxy-kubernetes-ingress-controller/

-- Installing HAProxy Ingress Controller into EKS:
https://www.haproxy.com/documentation/kubernetes/latest/installation/community/aws/

-- Options (annotations) for HAProxy 'Service' and 'Ingress' definitions:
https://www.haproxy.com/documentation/kubernetes/latest/configuration/service/

Useful ones (Ingress):
* haproxy.org/path-rewrite: (.*) /foo\1
* haproxy.org/request-set-header: Ingress-ID abcd123
* haproxy.org/set-host: "example.local"
* annotations:
    haproxy.org/path-rewrite: "/"                        # replace all paths with /
    haproxy.org/path-rewrite: (.*) /foo\1                # add the prefix /foo... "/bar?q=1" into "/foo/bar?q=1"
    haproxy.org/path-rewrite: ([^?]*)(\?(.*))? \1/foo\2  # add the suffix /foo ... "/bar?q=1" into "/bar/foo?q=1"
    haproxy.org/path-rewrite: /foo/(.*) /\1              # strip /foo ... "/foo/bar?q=1" into "/bar?q=1"

Troubleshooting:
kubectl logs <pod>

Install with debug logs:
helm install haproxy haproxytech/kubernetes-ingress \
  --set controller.logging.level=debug

View source YAML:
kubectl get deployment <deployment> -o yaml

-- 'Plain vanilla' install of HAProxy Ingress Controller

helm install devops-poc-ingress -n devops-poc \
    --set controller.logging.level=debug \
    --set controller.service.type=LoadBalancer \
    --set-string "controller.config.ssl-redirect=false" \
    haproxytech/kubernetes-ingress

-- HAProxy deployment with additional annotations:
helm install haproxy-controller \
Haproxyteach/kubernetes-ingress  \
  --set controller.service.type=LoadBalancer \
  --set controller.service.annotations.“servcie\.beta\.kubernetes\.io/aws-load-balancer-internal”=“0.0.0.0/0” \
  --set controller.service.annotations.”service\.beta\.kubernetes\.io/aws-load-balancer-cross-zone-load-balancing-enabled”=“true”

-- Troubleshooting HAProxy Ingresss Controller:
https://www.haproxy.com/documentation/kubernetes/latest/usage/troubleshooting/

Getitng logs from elements in Kubernetes:
kubectl logs -n kube-system   deployment.apps/aws-load-balancer-controller

Send request with explicit host setting:
curl -I -H 'Host: foo.bar' 'http://192.168.99.100:30279'

-- Test local web container using docker swarm:

docker swarm init 
docker service create --publish 8080 jmalloc/echo-server

-- ClusterRole with multiple ApiGroups:

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: get-pods
rules:
 - apiGroups: ["*"]
   resources: ["pods"]
   verbs: ["list","get","watch"]
 - apiGroups: ["extensions","apps"]
   resources: ["deployments"]
   verbs: ["get","list","watch","create","update","patch","delete"]

-- Mounting EFS file system:
sudo mount -t nfs -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-5c588564.efs.ap-southeast-2.amazonaws.com:/ efs

-- Mounting EFS Access Point:
mount -t efs -o tls,accesspoint=fsap-12345678 fs-12345678: /localmountpoint

-- Install EFS CSI driver into Kubernetes (for accessing PersistentVolumes with CSI driver instead of NFS)
kubectl apply -k "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-1.0"

-- Mounting EFS subdirectory as PV using CSI driver:
https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/volume_path/README.md 

* Learn EKS standup with Terraform:
https://learn.hashicorp.com/tutorials/terraform/eks

If you're running the AWS CLI version 1.16.156 or later, then you don't need to install the authenticator. Instead, you can use the aws eks get-token command. For more information, see Create kubeconfig manually. 

* Install EFS CSI driver to EKS cluster (so EFS can be mounted)
https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html

* Attach EFS to EKS with Terraform:
https://stackoverflow.com/questions/60609820/how-to-use-amazon-efs-with-eks-in-terraform

* Getting information from containers that are stuck in 'init' stage:
kubectl get pod nginx --template '{{.status.initContainerStatuses}}'

===================== STUFF FOR SAMPLE JENKINS PIPELINE ========================

STEPS FROM "SERVICE_PURCHASE":

#
language: ruby
cache: bundler

rvm:
  - 2.5.1

addons:
  postgresql: "9.4" # (...or later)

# http://blog.travis-ci.com/2014-12-17-faster-builds-with-container-based-infrastructure/
#
sudo: false

# Fix for bundler 2.0 (which tries to exec the exact version listed in Gemfile.lock under BUNDLED WITH)
# the travis test runner probably doesn't have this installed. We'll install it and rubygems 3.0+
# see: https://bundler.io/v2.0/guides/bundler_2_upgrade.html
before_install:
  - gem update --system
  - bundler_version=$(grep -A 1 "BUNDLED WITH" Gemfile.lock | sed -n 2p | tr -d ' ') && gem install bundler --version "${bundler_version}"
  - gem --version
  - gem list bundler

before_script:
  - psql -c 'create database service_purchase_test;' -U postgres

script:
  - RACK_ENV=test bundle exec rake db:migrate --trace
  - "bundle exec bundle-audit update && bundle exec bundle-audit check"
  - TZ=Pacific/Auckland bundle exec rspec spec --backtrace

# GEMFILE.LOCK BITS

GIT
  remote: https://github.com/pond/sdoc.git
  revision: e1e35566f9f207bffb3511fea4779629de94d029
  branch: master
  specs:
    sdoc (1.0.1)
      json
      rdoc (>= 50)

GIT
  remote: https://[token]@github.com/LoyaltyNZ/loyalty_platform_support.git
  revision: b31d09511e38ccc62ea5b05db4c23c7194f5d187
  branch: master
  specs:
    loyalty_platform_support (3.5.4)
      hoodoo (>= 1.12, < 3.0)

# DOCKERFILE BITS

# The "-stretch" suffix is to deal with:
#
#   https://trello.com/c/G4Ud3pT4/226-install-curl-latest-version-on-all-base-images
#
# ...and should probably be removed, along with this comment, whenever the
# Ruby base version is next updated.
#
FROM quay.io/loyalty_nz/service_base_ruby:2.5.1-stretch

MAINTAINER Loyalty Developers <developers@loyalty.co.nz>

COPY ./Gemfile* /www/
RUN bundle install --deployment --without development test

ADD . /www

# Run Bundler

CMD bundle exec rackup -s alchemy


Thoughts for ER:

[for x in aws_instance.example: x.id if x.availability_zone == "us-east-1a"]

locals {
  # flatten ensures that this local value is a flat list of objects, rather
  # than a list of lists of objects.
  network_subnets = flatten([
    for network_key, network in var.networks : [
      for subnet_key, subnet in network.subnets : {
        network_key = network_key
        subnet_key  = subnet_key
        network_id  = aws_vpc.example[network_key].id
        cidr_block  = subnet.cidr_block
      }
    ]
  ])
}

resource "aws_subnet" "example" {
  # local.network_subnets is a list, so we must now project it into a map
  # where each key is unique. We'll combine the network and subnet keys to
  # produce a single unique key per instance.
  for_each = {
    for subnet in local.network_subnets : "${subnet.network_key}.${subnet.subnet_key}" => subnet
  }

  vpc_id            = each.value.network_id
  availability_zone = each.value.subnet_key
  cidr_block        = each.value.cidr_block
}

Notes for Handover:

- infrastructure and configuration are co-mingled: more flexible to have configuration separate
- keys are stored in source code *everywhere*
- secrets management can be emphasized
- TerraformCloud is more of a hindrance than a help
- DevOps preso
- Introduce 'Architectural Decision' process: template below -- new ideas much be presented with other optons considered and pros/cons

Confluence Page:
https://loyaltynz.atlassian.net/wiki/spaces/TECH/pages/848297985/Architectural+Decision+Template
* attach example PDF

NOTES FOR JEREMY:

Migraton of platform POC from NeuralEssence environment to Loyalty: several changes added work items that were not expected
* VPC using exisitng code caused EKS not to function
* EKS built with exisitng code
* Charts deployed via Terraform rather than command-line
* Jenkins pipelines require a DB (postgres), was not part of the original plan



NOTES for SUMMARY:

Recommendations:
* review tips and tricks for minimizing size of Docker iamges -- many of these are bigger than they need to be



Hi Natasha and Mike,

We are looking at consolidating and simplifying the CI/CD processes at Loyalty to allow more flexibility to developers and expand our current capabilities.  As a 'first cab off the rank' I've selected the 'service_fly_buys_programme' build to migrate to Jenkins for comparison/contrast.  I'd like to ask your help in reverse-engineering what TravisCI & Quay.io are doing into a sequential set of steps with any dependencies and other 'Travis Magic' identified.

What I've come up with so far is:

1) clone codebase
2) run 'before-install' steps
3) ? is something done for 'install' phase?
4) create and populate a temp database (using Travis default Postgress) in 'before-script' steps
5) execute 'script' steps
6) hand over to Quay.io to build/tag/push Docker image

Couple of questions:
* is there anything missing that either TravisCI ro Quay.ip 'do for you' that isn't obvious?
* is the database that gets created a throw-away used only for testing?
* where does the tag come from that gets applied to the Docker iamge?
* does the 'loyalty_base_ruby' Dockedr image have tools to build as well as run the app (could simplify the Jenkins pipeline)?
* does Loyalty run all containers as 'root' user?


~/domains/neuralessence.com/uploads/Abundance_Email-06-Dec-2020.tgz

COMMANDS for MOUNTING AN ACCESS POINT:
sudo yum install -y amazon-efs-utils
sudo mount -t efs -o tls,accesspoint=[access-point-id] [file-system-id]: /[local mount point]
sudo mount -t efs -o tls,accesspoint=fsap-0b2ff1d26ae0ca2a1 fs-920cabaa: /jenkins-dev-home

WITHOUT EFS MOUNT HELPER:
sudo mount -t nfs -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport [file-system-id].efs.[aws-region].amazonaws.com:/ [efs-mount-point/]
sudo mount -t nfs -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-920cabaa.efs.ap-southeast-2.amazonaws.com:/ eks-test/

DOMAIN: loyaltydevops.co.nz

CHECKLIST to ADD TO DEVOPS EKS CLUSTER:
* Log Group (CloudWatch)
* What order to install add-ons?  Helm deploys?
* Add CloudWatch Logging (need lambda)

QUESTIONS FOR DEVOPS EKS CLUSTER:
* eks_public_access_cidrs?  Set to 0.0.0.0./0?
* where do eks_worker_public_key come from?
* How robust do these worker groups need to be?

git push -u origin <branch>

Notes:

* Backend config for Terraform if running locally

bucket               = "loyaltynz-terraform-state"
region               = "ap-southeast-2"
dynamodb_table       = "terraform-statelock"
workspace_key_prefix = "workspace"

https://s3.console.aws.amazon.com/s3/buckets/loyaltynz-terraform-state?region=ap-southeast-2 in the main account

data "aws_vpc" "eks" {
  filter {
    name   = "tag:Name"
    values = ["${terraform.workspace}"]
  }
}

That’s an example. Official doc: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/vpc

resource "aws_security_group" "eks_control_plane" {
  name        = "${terraform.workspace} - EKS Control Plane SG"
  description = "${terraform.workspace} EKS control plane security group"
  vpc_id      = "${data.aws_vpc.kubernetes_vpc.id}"

  tags = "${merge(
            local.tags,
            map(
              "Name", "${terraform.workspace}-eks-control-plane-sg",
              )
            )
          }"
}

## This stack depends on the vpc terraform stack being created first.
# This file loads the required AWS resources created by that stack

data "aws_vpc" "kubernetes_vpc" {
  tags = {
    Name       = "${terraform.workspace}"
    managed-by = "terraform"
  }
}

data "aws_subnet_ids" "eks_private" {
  vpc_id = "${data.aws_vpc.kubernetes_vpc.id}"

  tags = {
    Name = "${terraform.workspace} - Private"
  }
}

data "aws_subnet_ids" "eks_public" {
  vpc_id = "${data.aws_vpc.kubernetes_vpc.id}"

  tags = {
    Name = "${terraformworkspace} - Public"
  }
}
