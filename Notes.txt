* Code Repo:
https://github.com/SecretlyElvis/eks-experiment.git

### HELM | Jenkins
Jenkins.io Installing Jenkins on Kubernetes:
https://www.jenkins.io/doc/book/installing/kubernetes/

https://bitnami.com/stack/jenkins/helm
https://medium.com/faun/deploying-and-scaling-jenkins-on-kubernetes-2cd4164720bd
https://medium.com/appfleet/how-to-set-up-jenkins-on-kubernetes-70f8eac3dc7e
https://hands-on-tech.github.io/2020/03/15/k8s-jenkins-example.html
https://cloud.google.com/solutions/jenkins-on-kubernetes-engine?hl=it
https://www.edureka.co/community/74612/how-to-configure-jenkins-environment-using-helm
https://docs.netapp.com/us-en/project-astra/pdfs/pages/solutions/jenkins-deploy-from-helm-chart.pdf

https://cloudacademy.com/course/introduction-to-helm-1034/helm-charts/

HELM | Nexus

https://blog.sonatype.com/how-to-use-nexus-repository-and-helm-for-ci/cd
https://help.sonatype.com/repomanager3/formats/helm-repositories
https://github.com/helm/charts/tree/master/stable/sonatype-nexus
https://stackoverflow.com/questions/52274286/run-nexus-in-kubernetes-cluster-using-helm
https://www.freshbrewed.science/getting-started-with-containerized-nexus/index.html
https://www.sonatype.com/press-release-blog/sonatype-adds-native-helm-support

* Learn EKS standup with Terraform:
https://learn.hashicorp.com/tutorials/terraform/eks

If you're running the AWS CLI version 1.16.156 or later, then you don't need to install the authenticator. Instead, you can use the aws eks get-token command. For more information, see Create kubeconfig manually. 

* Install EFS CSI driver to EKS cluster (so EFS can be mounted)
https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html

kubectl apply -k "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-1.0"

* Attach EFS to EKS with Terraform:
https://stackoverflow.com/questions/60609820/how-to-use-amazon-efs-with-eks-in-terraform

module.vpc.aws_subnet.private[0]

@BMW had it right, I was able to get this all into Terraform.

In the aws/ directory I created all my AWS resources, VPC, EKS, workers, etc. and EFS mounts.

resource "aws_efs_file_system" "example" {
  creation_token = "${var.cluster-name}-example"

  tags = {
    Name = "${var.cluster-name}-example"
  }
}

resource "aws_efs_mount_target" "example" {
  count = 2
  file_system_id = aws_efs_file_system.example.id
  subnet_id = aws_subnet.this.*.id[count.index]
  security_groups = [aws_security_group.eks-cluster.id]
}

I also export the EFS file system IDs from the AWS provider plan.

output "efs_example_fsid" {
  value = aws_efs_file_system.example.id
}

After the EKS cluster is created I had to manually install the EFS CSI driver into the cluster before continuing.

Then in the k8s/ directory I reference the aws/ state file so I can use the EFS file system IDs in the PV creation.

data "terraform_remote_state" "remote" {
  backend = "s3"
  config = {
    bucket = "example-s3-terraform"
    key    = "aws-provider.tfstate"
    region = "us-east-1"
  }
}

Then created the Persistent Volumes using the Kubernetes provider.

resource "kubernetes_persistent_volume" "example" {
  metadata {
    name = "example-efs-pv"
  }
  spec {
    storage_class_name = "efs-sc"
    persistent_volume_reclaim_policy = "Retain"
    capacity = {
      storage = "2Gi"
    }
    access_modes = ["ReadWriteMany"]
    persistent_volume_source {
      nfs {
        path = "/"
        server = data.terraform_remote_state.remote.outputs.efs_example_fsid
      }
    }
  }
}

#### Resources not used in the final implementation ####

- Install EFS CSI driver into Kubernetes (for accessing PersistentVolumes with CSI driver instead of NFS)
kubectl apply -k "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-1.0"

- INSTALL NEXUS

## Variables
To be able to use this package, the following variables should be set.
see values.yaml

## Chart Details
This chart uses the docker image from [sonatype](https://github.com/sonatype/docker-nexus3).

## Installing the chart

## Configuration
The following tables lists the configurable parameters of the Nexus chart and their default values.

|Parameter|Description|Default|
|---------|-----------|-------|
|`hostName`|Host Name of this Instance|`nexus.example.com`|
|`containerPort`|Port of the container|`8081`|
|`maxMem`|Resource limit memory (-Xmx)|`1200M`|
|`minMem`|Minimum Memory (-Xms)|`1200M`|
|`javaOpts`|Additional options for the JVM|''|
|`persistence.path`|Path where all data on the host is stored|`/data/nexus`|

Specify each parameter using the --set key=value[,key=value] argument to helm install.

Alternatively, a YAML file that specifies the values for the parameters can be provided while installing the chart. For example,

```bash
$ helm install --name my-nexus -f values.private.yaml nexus-x.x.x.tgz
```

> **Tip**: You can use the default [values.yaml](values.yaml)

The initial Username/Pasword combination for the first login is: `admin/admin123`.

## Persistence
To be able to keep stateful data in the nexus kubernetes container, the following path is used:

```
/data/nexus
```

Right now, we do use HostPathes, which do not work in a real cluster environment (like AWS or GCE). Please adopt this one to your own needs.

The above mentioned path should be read/writable on the host for the user 1000.